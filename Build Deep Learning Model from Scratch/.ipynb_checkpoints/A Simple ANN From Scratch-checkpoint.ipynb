{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56abefa7",
   "metadata": {},
   "source": [
    "# Create an ANN from Scratch\n",
    "In this project, we will build a neural network from scratch (without using any Keras functionality) by applying the mathematics of deep learning. Finally, we'll implement it in image classification task on MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2369ce0",
   "metadata": {},
   "source": [
    "## The Dense Layer\n",
    "The Dense layer implements the following input transformation: \n",
    "\n",
    "**output = activation(dot(input, W) + b)**\n",
    "\n",
    "where W and b are model parameters - W is the connection matrix and b the bias added to each neuron, and activation is an element-wise function (usually relu, but it would be softmax for the last layer). Let’s implement a simple Python class, NaiveDense, that creates two TensorFlow variables, W and b, and exposes a call() method that applies the preceding transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c25676df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activaton):\n",
    "        self.activaton = activaton\n",
    "        \n",
    "        #Create a matrix, W\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "        \n",
    "        #Create a vector, b\n",
    "        b_shape = (output_size, )\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "        \n",
    "    #forward pass\n",
    "    def __call__(self, inputs):\n",
    "        return self.activaton(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b3f08",
   "metadata": {},
   "source": [
    "create a NaiveSequential class to chain these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1386296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights += layer.weights\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10334641",
   "metadata": {},
   "source": [
    "Now simulate a ANN model with activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edebd888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 16:48:54.766851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28*28, output_size=512, activaton=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activaton=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e91d0",
   "metadata": {},
   "source": [
    "Create a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bce949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "        \n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f663e0",
   "metadata": {},
   "source": [
    "Let's create one single training step including:\n",
    "1. Compute the predictions of the model for the images in the batch.\n",
    "2. Compute the loss value for these predictions, given the actual labels.\n",
    "3. Compute the gradient of the loss with regard to the model’s weights.\n",
    "4. Move the weights by a small amount in the direction opposite to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cb5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "#move the weights by “a bit” in a direction that will reduce the loss on this batch\n",
    "def update_weights(gradients, weights): \n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate)\n",
    "        \n",
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "        \n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc60c6ec",
   "metadata": {},
   "source": [
    "Define the full training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db09e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128): \n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "    batch_generator = BatchGenerator(images, labels)\n",
    "    for batch_counter in range(batch_generator.num_batches):\n",
    "        images_batch, labels_batch = batch_generator.next()\n",
    "        loss = one_training_step(model, images_batch, labels_batch) \n",
    "        if batch_counter % 100 == 0:\n",
    "            print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83fe38",
   "metadata": {},
   "source": [
    "## Test the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c275619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "loss at batch 0: 4.36\n",
      "loss at batch 100: 2.28\n",
      "loss at batch 200: 2.24\n",
      "loss at batch 300: 2.11\n",
      "loss at batch 400: 2.26\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49dac7d",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63d8fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy() \n",
    "predicted_labels = np.argmax(predictions, axis=1) \n",
    "matches = predicted_labels == test_labels \n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460046c3",
   "metadata": {},
   "source": [
    "## Exkurs\n",
    "If we utilize the Keras Layer subclass, we do not have to explicitly specify the input and output shapes of each layer. The layer itself will infer the shape. We would build a simple dense model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c87ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815da01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        \n",
    "    #weight creation\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units), initializer='random_normal')\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer='zeros')\n",
    "        \n",
    "    #forward pass\n",
    "    def call(self, inputs):\n",
    "        y = tf.matmul(inputs, self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
